Title : Facebook Doesn’t Tell Users Everything It Really Knows About Them — ProPublica
 
Original : 
Machine Bias Investigating Algorithmic Injustice
Facebook has long let users see all sorts of things the site knows about them, like whether they enjoy soccer, have recently moved, or like Melania Trump.
But the tech giant gives users little indication that it buys far more sensitive data about them, including their income, the types of restaurants they frequent and even how many credit cards are in their wallets.
Since September, ProPublica has been encouraging Facebook users to share the categories of interest that the site has assigned to them. Users showed us everything from “Pretending to Text in Awkward Situations” to “Breastfeeding in Public.” In total, we collected more than 52,000 unique attributes that Facebook has used to classify users.
Get the Data From This Story Download the Facebook interest category and ad group data ProPublica collected to report this story, available now via the ProPublica Data Store.
Facebook’s page explaining “what influences the ads you see” says the company gets the information about its users “from a few different sources.”
What the page doesn’t say is that those sources include detailed dossiers obtained from commercial data brokers about users’ offline lives. Nor does Facebook show users any of the often remarkably detailed information it gets from those brokers.
“They are not being honest,” said Jeffrey Chester, executive director of the Center for Digital Democracy. “Facebook is bundling a dozen different data companies to target an individual customer, and an individual should have access to that bundle as well.”
When asked this week about the lack of disclosure, Facebook responded that users can discern the use of third-party data if they know where to look. Each time an ad appears using such data, Facebook says, users can click a button on the ad revealing that fact. Users can still not see what specific information about their lives is being used.
The company said it does not disclose the use of third-party data on its general page about ad targeting because the data is widely available and was not collected by Facebook.
“Our approach to controls for third-party categories is somewhat different than our approach for Facebook-specific categories,” said Steve Satterfield, a Facebook manager of privacy and public policy. “This is because the data providers we work with generally make their categories available across many different ad platforms, not just on Facebook.”
Satterfield said users who don’t want that information to be available to Facebook should contact the data brokers directly. He said users can visit a page in Facebook’s help center, which provides links to the opt-outs for six data brokers that sell personal data to Facebook.
Limiting commercial data brokers’ distribution of your personal information is no simple matter. For instance, opting out of Oracle’s Datalogix, which provides about 350 types of data to Facebook according to our analysis, requires “sending a written request, along with a copy of government-issued identification” in postal mail to Oracle’s chief privacy officer.
Users can ask data brokers to show them the information stored about them. But that can also be complicated. One Facebook broker, Acxiom, requires people to send the last four digits of their social security number to obtain their data. Facebook changes its providers from time to time so members would have to regularly visit the help center page to protect their privacy.
One of us actually tried to do what Facebook suggests. While writing a book about privacy in 2013, reporter Julia Angwin tried to opt out from as many data brokers as she could. Of the 92 brokers she identified that accepted opt-outs, 65 of them required her to submit a form of identification such as a driver’s license. In the end, she could not remove her data from the majority of providers.
ProPublica’s experiment to gather Facebook’s ad categories from readers was part of our Black Box series, which explores the power of algorithms in our lives. Facebook uses algorithms not only to determine the news and advertisements that it displays to users, but also to categorize its users in tens of thousands of micro-targetable groups.
Our crowd-sourced data showed us that Facebook’s categories range from innocuous groupings of people who like southern food to sensitive categories such as “Ethnic Affinity” which categorizes people based on their affinity for African-Americans, Hispanics and other ethnic groups. Advertisers can target ads toward a group — or exclude ads from being shown to a particular group.
Last month, after ProPublica bought a Facebook ad in its housing categories that excluded African-Americans, Hispanics and Asian-Americans, the company said it would build an automated system to help it spot ads that illegally discriminate.
Facebook has been working with data brokers since 2012 when it signed a deal with Datalogix. This prompted Chester, the privacy advocate at the Center for Digital Democracy, to file a complaint with the Federal Trade Commission alleging that Facebook had violated a consent decree with the agency on privacy issues. The FTC has never publicly responded to that complaint and Facebook subsequently signed deals with five other data brokers.
To find out exactly what type of data Facebook buys from brokers, we downloaded a list of 29,000 categories that the site provides to ad buyers. Nearly 600 of the categories were described as being provided by third-party data brokers. (Most categories were described as being generated by clicking pages or ads on Facebook.)
The categories from commercial data brokers were largely financial, such as “total liquid investible assets $1-$24,999,” “People in households that have an estimated household income of between $100K and $125K,” or even “Individuals that are frequent transactor at lower cost department or dollar stores.”
We compared the data broker categories with the crowd-sourced list of what Facebook tells users about themselves. We found none of the data broker information on any of the tens of the thousands of “interests” that Facebook showed users.
Our tool also allowed users to react to the categories they were placed in as being “wrong,” “creepy” or “spot on.” The category that received the most votes for “wrong” was “Farmville slots.” The category that got the most votes for “creepy” was “Away from family.” And the category that was rated most “spot on” was “NPR.”
Clarification, Jan. 4, 2017: We’ve added details about what Facebook tells users regarding third-party data. Specifically, each time an ad appears using such information, Facebook says, users can click a button on the ad revealing the use of third-party data.

Generated : 
Every week, Facebook messages from Facebook Friends and family show up on our news feeds. But a new report by Google’s Paquet reports that users may have been telling users everything they knew about them, including “how to interact with a woman.”
Paquet found that it was the algorithm that created the messages sent by 23,482 likes, 279 comments, 555 comments, 101 replies, 66 replies, and over 965,000 comments for about a third of the users analyzed. It looked for correlations between user presence and length of a social interaction. Users mentioned in 107 percent of the posts were open or talked about the same person or two.
Ben Steelman, a specialist in the internet of things, told Paquet that people’s reactions to what they see and hear are also marked by content, and that when it comes to what happens to people, there are very few patterns.
This is important because information is often filtered by the algorithms that control it. When they present an appeal to a user, they say, it’s a positive that they “like,” thus generating more of the appeal. “The algorithm works like that,” he added.
Here’s the crux of the controversy: when people have been offered information about a person on Facebook, they say something like, “Thank you.”
But when they are offered information about themselves, they say something like, “I like and I don’t like him.”
This is problematic for a vast number of reasons, Paquet found. There’s a host of factors as to why people would give themselves a favorable impression of a person — including personality, language, relationships, motivation, etc. When people try to expand their ego they get more negative feedback, and share certain material they don’t really want to be seen for.
Steelman thinks that attempts to give this information to users might be being driven by the algorithm.
“It is a tool for generating lots of emotions. It doesn’t really have anything to do with the algorithm in general,” he said.
“For instance, if we look at how people talk with the way they describe themselves, why would they give what they thought they had, or if there is an anomaly, why do they think that it’s not generating more positive feelings?”
But, Steelman added, if you expect to know what’s going on in a person’s life, you would need to be familiar with who they are and what they are going through in order to apply this information to the person’s life, and not “wholly,” as Paquet puts it, “just to get in a cut-and-paste situation.”
The answer from Paquet appears to be a simple one: it works to change the context in which the algorithm is manipulating. It forces friends and family to become associates. But this looks to be only one example of a trend that is altering people’s view of themselves. But it’s probably more of a thing than that, since it shows that Facebook’s algorithms are continually increasing the length of their interactions with people they already know.
“People’s Facebook impressions — which by default are always skewed in the right direction — is being replaced,” he said.
